\chapter{Method}\label{ch:method}

As summarised in Chapter \ref{ch:lit_rev}, there exist a number of different
data assimilation schemes, many of which are used extensively in fields such as
numerical weather prediction, navigation and tracking, and other forecasting
problems.
Such methods, however, have been sparsely used in the field of real-time urban
simulation and as such this investigation attempts to build upon the existing
work by implementing the Ensemble Kalman Filter in conjunction with a pedestrian
agent-based model.
This chapter therefore seeks to outline the method used in this investigation --- the
Ensemble Kalman Filter. 
As shall be explained, the Ensemble Kalman Filter is an approximation of the
Kalman Filter, and as such some attention will first be given to the original
Kalman Filter, followed by an explanation of the Ensemble Kalman Filter along
with the innovations that it incorporates.

\section{Kalman Filter}\label{sec:method:kf}

%One of the earliest forms of Bayesian filtering is known as Wiener filtering
%\citep{wiener1950extrapolation}, which is used in the field of signal
%processing.
One of the earliest forms of Bayesian filtering is the sequential data
asssimilation scheme known as the Kalman Filter, which forms the foundation of
this piece of work.
As with other sequential data assimilation schemes, the Kalman Filter operates
on a model with a given state and forecasting process, updating the state and
associated covariance matrix upon receipt of new observations.
As outlined in Section \ref{sec:lit_rev:da}, this can be encoded in Bayes
Theorem:
\begin{equation}
    P \left( \mathbf{x} | \mathbf{d} \right) = 
    \frac{P ( \mathbf{d} | \mathbf{x} ) P ( \mathbf{x} )}{P ( \mathbf{d} ) }.
\end{equation}

\begin{equation}
    \hat{\mathbf{x}} = \mathbf{x} + \mathbf{K} \left(
                        \mathbf{d} - \mathbf{H} \mathbf{x} \right)
\end{equation}

\begin{equation}
    \hat{\mathbf{Q}} = \left( \mathbf{I} - \mathbf{K} \mathbf{H} \right)
                        \mathbf{Q}
\end{equation}

\begin{equation}
    \mathbf{K} = \mathbf{Q} \mathbf{H}^T \left(
                    \mathbf{H} \mathbf{Q} \mathbf{H}^T + \mathbf{R}
                 \right) ^ {-1}
\end{equation}

This update process is undertaken based on the uncertainty in the model
forecasts and the observation uncertainty with a view to minimising the
posterior mean squared error.

\begin{itemize}
    \item When is it bad?
    \item What can we do to improve it?
    \item The KF provides us with the exact posterior estimate when errors are
        normally distributed.
    \item KF is based on linear dynamical system --- that is to say that both of
        the matrices $\mathbf{M}$ (the model forecast operator) and $\mathbf{H}$
        (the observation operator) perform linear transformations.
    \item This is often not the case with more complex systems, particularly
        when the system elements interact with each other (as is typically the
        case in Agent-Based Models).
    \item 
\end{itemize}

\begin{enumerate}
    \item Predict
    \item Update
\end{enumerate}

\section{Ensemble Kalman Filter}\label{sec:method:enkf}

Problems with the Kalman Filter:
\begin{itemize}
    \item it assumes Gaussian PDFs
    \item it assumes linear model
    \item Cost of evolving covariance matrix
\end{itemize}

In order to address some of these problems, the Ensemble Kalman Filter was
developed \citep{evensen2003ensemble, evensen2009ensemble}, which acts as an
approximation of the Kalman Filter.
This approximation is achieved through a Monte Carlo approach of using an
ensemble of sample state vectors to represent the state distribution; this
development mirrors the recent incorporation of Monte Carlo methods in the field
of Bayesian statistics \citep{wikle2007bayesian}.
As such, the state is represented as follows:
\begin{equation}
    \mathbf{X} = \left[ \mathbf{x}_1, \ldots, \mathbf{x}_N \right]
               = \left[ \mathbf{x}_i \right], \quad \forall i \in (1, N),
\end{equation}
where the state ensemble matrix, $\mathbf{X}$, consists of $N$ state vectors,
$\mathbf{x}_i$.
The mean state vector, $\bar{\mathbf{x}}$, can be found by averaging over the
ensemble:
\begin{equation}
    \bar{\mathbf{x}} = \sum_{i = 1}^{N} \mathbf{x}_i.
\end{equation}
Similarly, the observations are represented as follows:
\begin{equation}
    \mathbf{D} = \left[ \mathbf{d}_1, \ldots, \mathbf{d}_N \right]
               = \left[ \mathbf{d}_i \right], \quad \forall i \in (1, N),
\end{equation}
with each member of the data ensemble matrix, $\mathbf{D}$, being the sum of the
original observation $\mathbf{d}$, and a random vector, $\mathbf{\epsilon}_i$:
\begin{equation}
    \mathbf{d}_i = \mathbf{d} + \mathbf{\epsilon}, \quad
                   \forall i \in (1, N).
\end{equation}
The random vector is drawn from an unbiased normal distribution:
\begin{equation}
    \mathbf{\epsilon} \sim \mathcal{N} (0, \mathbf{R}).
\end{equation}
As with the model state, the mean data vector, $\bar{\mathbf{d}}$, can be found
by averaging over the ensemble:
\begin{equation}
    \bar{\mathbf{d}} = \sum_{i = 1}^{N} \mathbf{d}_i.
\end{equation}
Given that the noise added to the original data vector is unbiased, we should
expect that
\begin{equation}
    \lim_{N \to \infty} \bar{\mathbf{d}} = \mathbf{d}
\end{equation}

Given the above framework, the data assimilation cycle is made up of:
\begin{enumerate}
    \item Predict
    \item Update:
    \begin{equation}
        \hat{\mathbf{X}} = \mathbf{X} + \mathbf{K}
                           \left(
                           \mathbf{D} - \mathbf{H} \mathbf{X}
                           \right),
    \end{equation}
    with the Kalman Gain Matrix being given by
    \begin{equation}
        \mathbf{K} = \mathbf{C} \mathbf{H}^T
                     {\left(
                     \mathbf{H} \mathbf{C} \mathbf{H}^T
                     + \mathbf{R}
                     \right)} ^ {-1}.
    \end{equation}
\end{enumerate}

%\subsection{Different Types of Ensemble Kalman Filter}\label{sec:method:types}

%Talk about the different types of EnKF and the implications for ensemble size
%\citep{keller2018comparing}.

%\begin{itemize}
    %\item Damping: counteract filter divergence
    %\item Localisation: reduce the effect of spurious correlations
    %\item Hybrid EnKF: Covariance matrix is made up of the weighted sum of the
        %usual covariance matrix and a separate static covariance matrix that
        %encodes prior underlying knowledge about the system
    %\item Dual EnKF: Split the state vector into state and parameters. At
        %assimilation: update parameters, recalculate forecast, update state
    %\item Normal Score EnKF: Developed to handle non-Gaussian PDFs in EnKF. At
        %assimilation: transform state, parameters and measurements into
        %Z-scores, perform EnKF update based on transformed values, transform
        %back from Z-scores
    %\item Iterative EnKF
%\end{itemize}
